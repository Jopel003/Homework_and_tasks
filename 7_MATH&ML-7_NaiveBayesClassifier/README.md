# Наивный байесовский классификатор: практика

## Оглавление  
<a id = '0'></a>
<a href ="#1">1. Описание работы</a><br>
<a href ="#2">2. Задачи</a><br>
<a href ="#3">3. Основные цели</a><br>
<a href ="#4">4. Этапы работы над проектом</a><br>
<a href ="#5">5. Результат</a><br>
<a href ="#6">6. Выводы</a><br>

### Описание проекта    
<a id = '1'></a>
Реализовать классификацию спам-сообщений электронной почты с использованием готовых функций.

:arrow_up:<a href ="#0">к оглавлению</a>


### Задачи    
<a id = '2'></a>
**Бизнес-задача**: определить характеристики и с их помощью спрогнозировать длительность поездки на такси.

**Техническая задача для вас как для специалиста в Data Science**: построить модель машинного обучения, которая на основе предложенных характеристик клиента будет предсказывать числовой признак — время поездки такси, то есть решить задачу регрессии.

### Основные цели:
<a id = '3'></a>
- Сформировать набор данных на основе нескольких источников информации.
- Спроектировать новые признаки с помощью Feature Engineering и выявить наиболее значимые при построении модели.
- Исследовать предоставленные данные и выявить закономерности.
- Построить несколько моделей и выбрать из них ту, которая показывает наилучший результат по заданной метрике.
- Спроектировать процесс предсказания длительности поездки для новых данных.
- Загрузить своё решение на платформу Kaggle, тем самым поучаствовав в настоящем Data Science-соревновании.

:arrow_up:<a href ="#0">к оглавлению</a>


### Этапы работы над проектом  
<a id = '4'></a>
1. **Первичная обработка данных**  
В рамках этой части предстоит сформировать набор данных на основе предложенных нами источников информации, а также обработать пропуски и выбросы в данных.
2. **Разведывательный анализ данных (EDA)**  
Необходимо будет исследовать данные, нащупать первые закономерности и выдвинуть гипотезы.
3. **Отбор и преобразование признаков**  
На этом этапе перекодируем и преобразуем данные таким образом, чтобы их можно было использовать при решении задачи регрессии — для построения модели.
4. **Решение задачи регрессии: линейная регрессия и деревья решений**  
На данном этапе построим свои первые прогностические модели и оценим их качество. Тем самым создадим так называемый baseline, который поможет ответить на вопрос: «Решаема ли вообще представленная задача?»
5. **Решение задачи регрессии: ансамбли моделей и построение прогноза**  
На заключительном этапе сможем доработать своё предсказание с использованием более сложных алгоритмов и оценить, с помощью какой модели возможно сделать более качественные прогнозы.

:arrow_up:<a href ="#0">к оглавлению</a>


### Результат:  
<a id = '5'></a>
Вспе получилось, использовали разные иснтрументы и модели лобучения.

Результат в цифрах:
* RMSLE для тренировочной 0.37
* RMSLE для валидационной 0.39
* Значение метрики MeAE 1.80

Также, в конце работы представлен бонус в виде примера использования модельи экстремального градиентного бустинга (XGBoost) из библиотеки xgboost.  
Возможно самостоятельно потренироваться в улучшении метрик.

:arrow_up:<a href ="#0">к оглавлению</a>


### Выводы:  
<a id = '6'></a>
Не стоит останавливайться на полученном решении этой задачи. Это лишь один из возможных вариантов. Стоит попробовать улучшить качество предсказания, используя более продвинутые подходы для генерации признаков, обработки пропусков, поиска выбросов, отбора признаков и так далее. 

Поэкспериментировать с методами оптимизации гиперпараметров алгоритмов. Но с осторожностью, так как в обучающем наборе очень много данных и подбор внешних параметров может занимать много времени. Будем выбирать диапазоны оптимизации с умом.

Также можно воспользоваться более сложными методами машинного обучения, например современными вариантами бустинга, такими как CatBoost от Яндекса или LightGBM от Microsoft.

Можно даже использовать стекинг, агрегировав несколько мощных моделей в одну.

Буду рад зведочке сверху ⭐️⭐️⭐️

:arrow_up:<a href ="#0">к оглавлению</a>
